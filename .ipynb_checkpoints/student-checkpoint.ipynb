{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31730a42",
   "metadata": {},
   "source": [
    "![example](images/pexels-pixabay-40568.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b9e86",
   "metadata": {},
   "source": [
    "# Phase 5 Project\n",
    "\n",
    "**Author:** Freddy Abrahamson<br>\n",
    "**Date created:** 8-14-2022<br>\n",
    "**Discipline:** Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22279f78",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c2967",
   "metadata": {},
   "source": [
    "### In order for the notebook to run successfully:\n",
    "\n",
    "1. Download the data from https://www.kaggle.com/datasets/andrewmvd/retinal-disease-classification\n",
    "2. The folder should be downloaded to the same directory as where your Jupyter notebook is located. The folder is called 'archive'.\n",
    "3. In order to recreate the necessary python environment, the environment.yml has been uploaded to github.\n",
    "4. The notebook was run on macOS Big Sur (version 11.6) \n",
    "\n",
    "### About Retinal Diseases\n",
    "According to the World Health Organization, atleast 2.2 billion people have some type of vision impairment, of which about half of these cases could have been prevented, or are yet to be addressed *. \n",
    "\n",
    "* https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "<b>Stakeholder:</b> The Board of directors of a national network of  eye hospitals.\n",
    "\n",
    "<b>Business Problem:</b> The hospital is looking for a solution that would enable the early detection and diagnosis of ocular diseases.  \n",
    "\n",
    "<b>Proposed Solution:</b> A machine learning model that could distinguish between the image of a healthy retina, and of an unhealthy one. It can also detect for media haze (cloudy vision), and diabetic retinopathy.\n",
    "\n",
    "<b>Solution Benefits:</b><br>\n",
    "               1. cost effective<br> \n",
    "               2. non-invasive<br>\n",
    "               3. would enable doctors to more effectively prevent, treat, and forestall the onset of ocular diseases\n",
    "\n",
    "### Data Understanding and Data preparation\n",
    "\n",
    "The data was taken from Kaggle.com. There are a total of 3200 images depicting healthy retinas, or retinas with a particular disease . The dataset is split into 3 subsets: training 60% (1920 images), evaluation 20% (640 images), and test 20% (640 images) sets. Each patient could potentially have more than one condition. I chose to work with the disease classes that 'occurred' in the training set at least 300 times.  Based on this criteria I was working with three different classes:<br><br>\n",
    "                                                   Disease_Risk:                    1519<br>\n",
    "                                                   DR (diabetic retinopathy):       376<br>\n",
    "                                                   MH (media haze):                 317<br>\n",
    "\n",
    "### Modeling\n",
    "\n",
    "I used Keras and Tensorflow to create the models. Given that with the use of the filters, cnn(s) excel at detecting features of various sizes,I chose to use the less apt multi-layer perceptron as a baseline model. My modeling process was the following:\n",
    "1. I tried to overfit on purpose using a cnn. Eventhough I was trying to overfit, I was also checking to see if that rise in the training score had any impact on the validation score. I began with a cnn model that has 4 activation layers for the feature extraction part,with the number of nodes for each layer being 16,32,64, and 128 respectively. I used ReLu as my activation function for all feature detection, as well as for the classification layers. Given that this is a a multi-label problem, I used three sigmoid functions in the output layer; one representing each class.<br><br>\n",
    "2. From there, when I was satisfied with the model architecture(number of layers, and number of nodes per layer), I  would try to reduce the variance by using different combinations of filter sizes, with the goal of making the model more generalizable.<br><br>\n",
    "3. Once I was satisifed with the filters, I would apply an optimizer with different learning rates, to see if this would improve results, or give me additional information about where the line plots for the scores of the respective data sets stopped oscillating, and possibly reach their respective convergences'.<br><br>\n",
    "4. After this, I would choose the best model amongst these and try to further reduce the variance by using some regularization technique, such as adding a dropout layer(s), or using L1, or L2 regularization. \n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Given the importance of early detection of potential ocular diseases, my primary goal was to find a model that produced the best recall scores. To this end, I used a custom metric, 'recall_macro'. This metric returns the macro average of the respective recall scores of each class. So in this case, the metric would take the recall score of each of the three classes, add them up, and divide by three. In order to find the best model I took several factors into consideration:<br>\n",
    "1. I was looking for a model that would produce one of the best,if not the best, bias/variance combination between the train and test data sets. I did this by creating a function best_model(), which utilizes the auc() function from sklearn.metrics. The x-axis is represented by the absolute difference between the train and test scores, while the y-axis is represented by the test scores. The higher the test score, and the lower the train-test difference, the greater the area under the curve. The function returns a dataframe with the models, and their respective test scores, sorted by their auc. The model with the highest auc is the best.<br><br>\n",
    "2. The difference between the train and validation scores was also important. Ideally there would be some uniformity across all three scores, <br><br>\n",
    "3. I also look at the recall/epoch plots to see if the train/val scores line up with any possible information that can be inferred from the plots.<br><br>\n",
    "The secondary goal was a model that would have a macro accuracy score of atleast 70%. To this end, I created a custom metric, 'acc_macro', which takes the accuracy scores for each individual class, and returns the macro average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f541b75",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996c0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import itertools\n",
    "import lime\n",
    "from lime import lime_image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "from keras import models, layers, optimizers\n",
    "from keras.preprocessing import image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7399f7",
   "metadata": {},
   "source": [
    "<b>Creating random seeds for reproducibility.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad1ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8dd22",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c8e07",
   "metadata": {},
   "source": [
    "## acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c560928",
   "metadata": {},
   "source": [
    "Takes as arguments:\n",
    " 1. array of labels\n",
    " 2. array of predictions\n",
    "    \n",
    "Returns the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5876e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Takes as arguments:\n",
    "    1. array of labels\n",
    "    2. array of predictions\n",
    "    \n",
    "    Returns the accuracy score.\n",
    "    \"\"\"\n",
    "    from keras import backend as K\n",
    "\n",
    "    # total = the total number of possible solutions:\n",
    "    total = y_true.shape[0]\n",
    "    # true positives (where both the actual value and the prdicted value equal 1)\n",
    "    tp = K.sum(K.round(y_true * y_pred))\n",
    "    # false positives (where the difference between the predicted value and the actual value equals 1) \n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))\n",
    "    # false negatives (where the difference between the actual value and the predicted value equals 1) \n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "    # true negatives (the difference between all four possible results, and the sum of other 3 possible results)\n",
    "    tn = total - (tp + fp + fn)\n",
    "    \n",
    "    # epsilon is a very small number. It will be added to the 'total', if it's = 0, so as not to return an error.\n",
    "    if total == 0:\n",
    "        return  (tp + tn)/(total + K.epsilon())\n",
    "    else:\n",
    "        return  (tp + tn)/ total "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecdeef",
   "metadata": {},
   "source": [
    "## acc_macro()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c499555",
   "metadata": {},
   "source": [
    "The acc_macro function measures the accuracy for each class, and then returns the macro average of all the classes. For example, if I have three classes A, B, and C. It would measure the average of each respective class, add them together, and divide by three. It is used as a custom metric when compiling the Keras models, but can also be used as a standalone function taking as its arguments the data-set labels, and the data-set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f270c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_macro(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The acc_macro function measures the accuracy for each class, and then returns the macro average of all the\n",
    "    classes. It work in conjunction with the 'acc' function. For example, if I have three classes A, B, and C. It \n",
    "    would measure the accuracy of each respective class, add them together, and divide by three. It is used as a\n",
    "    custom metric when compiling the Keras models, but can also be used as a stand-alone function taking as its\n",
    "    arguments the data-set labels, and the data-set predictions. The definition is set up for a three class array.\n",
    "    It would need to be modified if the number of classes were changed.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (acc(y_true[:,0],y_pred[:,0]) + acc(y_true[:,1],y_pred[:,1])\n",
    "    + acc(y_true[:,2],y_pred[:,2]))/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113d814",
   "metadata": {},
   "source": [
    "## best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5ba37",
   "metadata": {},
   "source": [
    "The best_model function returns the best train test score combination based on the auc function, where\n",
    "the difference between the test and the train scores represents the x axis, and test score represents\n",
    "the y axis. In order use the auc function, for each x,y coordinate we created a list of length three,\n",
    "with 0 and 1 at the ends, and the actual x,y values in the middle. The model with the highest auc score is the best. \n",
    "\n",
    "This function takes as as an argument an integer which represents the total number of models.\n",
    "\n",
    "\n",
    "This function returns a dataframe with five columns:\n",
    "  1. The model name\n",
    "  2. The recall score for the train set\n",
    "  3. The recall score for the test set\n",
    "  4. The absolute value of the difference between the two scores\n",
    "  5. The auc score, sorted in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab45f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(n):\n",
    "    \n",
    "    \"\"\"\n",
    "    The best_model function returns the best train test score combination based on the auc function, where\n",
    "    the difference between the test and the train scores represents the x axis, and test score represents\n",
    "    the y axis. In order use the auc function, for each x,y coordinate we created a list of length three,\n",
    "    with 0 and 1 at the ends, and the actual x,y values in the middle. The model with the highest auc score is\n",
    "    the best. \n",
    "\n",
    "    This function takes as as an argument an integer which represents the total number of models.\n",
    "\n",
    "\n",
    "    This function returns a dataframe with eight columns:\n",
    "    1. The model name\n",
    "    2. The recall score for the train set\n",
    "    3. The recall score for the validation set\n",
    "    4. The recall score for the test set\n",
    "    5. The absolute value of the difference between the train and the test scores.\n",
    "    6. The absolute value of the difference between the train and the validation scores.\n",
    "    7. The average of the train/val difference, and the train/test difference.\n",
    "    8. The auc score, sorted in ascending order\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import auc\n",
    "    scores_df = neural_network_model_scores_df(n)\n",
    "\n",
    "#   creating 'test_scores' and 'score_diffs' zero populated lists of shape(rows,3) \n",
    "    rows = len(scores_df)\n",
    "    test_scores = np.zeros((rows, 3))\n",
    "    score_diffs = np.zeros((rows, 3))\n",
    "    auc_scores = []\n",
    "\n",
    "#   populating 'test_scores' and 'score_diffs' so each list has a format [0,test_score,1],\n",
    "#   and [0,score_diff,1] respectively\n",
    "    for row in range(rows):\n",
    "        test_scores[row][1] = scores_df['test score'][row]\n",
    "        test_scores[row][2] = 1\n",
    "        score_diffs[row][1] = scores_df['train-test diff'][row]\n",
    "        score_diffs[row][2] = 1\n",
    "\n",
    "#   creating a list of all the auc scores\n",
    "    for row in range(rows):\n",
    "        auc_score = auc(score_diffs[row], test_scores[row])\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "#   getting the greatest auc score, and the index number of that row    \n",
    "    best_auc_score = max(auc_scores)\n",
    "    best_score_index = auc_scores.index(best_auc_score)\n",
    "    \n",
    "#   add an auc_score cloumn to scores_df\n",
    "    scores_df['auc score'] = auc_scores\n",
    "    \n",
    "\n",
    "#   return scores_df sprted by auc score\n",
    "    return scores_df.sort_values(by='auc score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93980a0f",
   "metadata": {},
   "source": [
    "## neural_network_model_scores_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1eb92",
   "metadata": {},
   "source": [
    "This function takes as as an argument an integer which represents the total number of models.\n",
    "\n",
    "This function returns a dataframe with four columns:\n",
    "   1. The model name\n",
    "   2. The recall score for the train set\n",
    "   3. The recall score for the validation set\n",
    "   4. The recall score for the test set\n",
    "   5. The absolute value of the difference between the train and the test scores.\n",
    "   6. The absolute value of the difference between the train and the validation scores.\n",
    "   7. The average of the train/val difference, and the train/test difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d098c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model_scores_df (n):\n",
    "    \"\"\"\n",
    "    This function takes as as an argument an integer which represents the total number of models.\n",
    "\n",
    "    This function returns a dataframe with seven columns:\n",
    "    1. The model name\n",
    "    2. The recall score for the train set\n",
    "    3. The recall score for the validation set\n",
    "    4. The recall score for the test set\n",
    "    5. The absolute value of the difference between the train and the test scores.\n",
    "    6. The absolute value of the difference between the train and the validation scores.\n",
    "    7. The average of the train/val difference, and the train/test difference.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    model_dict ={}\n",
    "    model_dict_list =[]\n",
    "    while count<n:\n",
    "        if count == 0:\n",
    "            train_score = globals()['baseline_train_eval_dict']['recall_macro']\n",
    "            val_score = globals()['baseline_val_eval_dict']['recall_macro']\n",
    "            test_score = globals()['baseline_test_eval_dict']['recall_macro']\n",
    "            model_name = 'baseline_model'\n",
    "            model_dict = {'model name':model_name, 'train score':train_score, 'val score':val_score,\n",
    "                          'test score':test_score}\n",
    "            model_dict_list.append(model_dict)\n",
    "            count+=1\n",
    "        else:\n",
    "            train_score = globals()['cnn_' +str(count) +'_train_eval_dict']['recall_macro']\n",
    "            val_score = globals()['cnn_' +str(count) +'_val_eval_dict']['recall_macro']\n",
    "            test_score = globals()['cnn_' +str(count) +'_test_eval_dict']['recall_macro']\n",
    "            model_name = 'cnn_model_'+ str(count)\n",
    "            model_dict = {'model name':model_name, 'train score':train_score, 'val score':val_score,\n",
    "                          'test score':test_score}\n",
    "            model_dict_list.append(model_dict)\n",
    "            count+=1\n",
    "    scores_df = pd.DataFrame(model_dict_list)\n",
    "    scores_df['train-val diff'] = abs(scores_df['train score'] - scores_df['val score'])\n",
    "    scores_df['train-test diff'] = abs(scores_df['train score'] - scores_df['test score'])\n",
    "    scores_df['train-val-test diff mean'] = (scores_df['train-val diff'] + scores_df['train-test diff'])/2\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee783a1",
   "metadata": {},
   "source": [
    "## plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5082446",
   "metadata": {},
   "source": [
    "Note: The code for this function was taken from the instructional website: https://deeplizard.com/learn/video/km7pxKy4UHU\n",
    "\n",
    "This function takes the following arguments:\n",
    "   1. cm: The array representing the results from the scikit-learn confusion_matrix() function\n",
    "   2. classes: an array with the name of the class labels of the confusion matrix.\n",
    "   3. normalize: if True, normalizes the values displayed by the confusion matrix. Default is False.\n",
    "   4. title: The title of the confusion matrix. Default is 'Confusion Matrix'.\n",
    "   5. cmap: Matplotlib colormap. Default is plt.cm.Blues.\n",
    "\n",
    "This function returns a graphical plot of the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e177757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Note: The code for this function was taken from the instructional website:\n",
    "    https://deeplizard.com/learn/video/km7pxKy4UHU\n",
    "\n",
    "    This function takes the following arguments:\n",
    "    1. cm: The array representing the results from the scikit-learn confusion_matrix() function\n",
    "    2. classes: an array with the name of the class labels of the confusion matrix.\n",
    "    3. normalize: if True, normalizes the values displayed by the confusion matrix. Default is False.\n",
    "    4. title: The title of the confusion matrix. Default is 'Confusion Matrix'.\n",
    "    5. cmap: Matplotlib colormap. Default is plt.cm.Blues.\n",
    "\n",
    "    This function returns a graphical plot of the confusion matrix. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c3e9c",
   "metadata": {},
   "source": [
    "## recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7be313",
   "metadata": {},
   "source": [
    " Takes as arguments:\n",
    " 1. array of labels\n",
    " 2. array of predictions   \n",
    "Returns the recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c0b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Takes as arguments:\n",
    "    1. array of labels\n",
    "    2. array of predictions\n",
    "    \n",
    "    Returns the recall score.\n",
    "    \"\"\"\n",
    "#(recall class 1) do not use \"round\" here if you're going to use this as a loss function\n",
    "    from keras import backend as K\n",
    "    true_positives = K.sum(K.round(y_pred) * y_true)\n",
    "    possible_positives = K.sum(y_true)\n",
    "    if possible_positives == 0:\n",
    "        return true_positives / (possible_positives + K.epsilon()) \n",
    "    else:\n",
    "        return true_positives / possible_positives\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcce274",
   "metadata": {},
   "source": [
    "## recall_macro()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83117a52",
   "metadata": {},
   "source": [
    "The recall_macro function measures the recall for each class, and then returns the macro average of all the\n",
    "classes. It works in conjunction with the 'recall' function. For example, if I have three classes A, B, and C. It \n",
    "would measure the recall of each respective class, add them together, and divide by three. It is used as a\n",
    "custom metric when compiling the Keras models, but can also be used as a standalone function taking as its\n",
    "arguments the data-set labels, and the data-set predictions. The definition is set up for a three class array.\n",
    "It would need to be modified if the number of classes were changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3bfbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_macro(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The recall_macro function measures the recall for each class, and then returns the macro average of all the\n",
    "    classes. It works in conjunction with the 'recall' function. For example, if I have three classes A, B, and C. It \n",
    "    would measure the recall of each respective class, add them together, and divide by three. It is used as a\n",
    "    custom metric when compiling the Keras models, but can also be used as a standalone function taking as its\n",
    "    arguments the data-set labels, and the data-set predictions. The definition is set up for a three class array.\n",
    "    It would need to be modified if the number of classes were changed.\n",
    "    \"\"\"\n",
    "    return (recall(y_true[:,0],y_pred[:,0]) + recall(y_true[:,1],y_pred[:,1])+ recall(y_true[:,2],y_pred[:,2]))/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a30d23",
   "metadata": {},
   "source": [
    "## recall_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21251fa",
   "metadata": {},
   "source": [
    "This function takes as an as an argument a dictionary returned by the evaluate() method from a tensorflow model, that includes recall, among it's metrics.\n",
    "\n",
    "This function returns the following:\n",
    "   1. The key/value pair corresponding to the recall_macro score\n",
    "   2. The key/value pair corresponding to the acc_macro score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b7cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_dict(eval_dict):\n",
    "    \"\"\"\n",
    "    This function takes as an as an argument a dictionary returned by the evaluate() method from a tensorflow model,\n",
    "    that includes recall, among it's metrics.\n",
    "\n",
    "    This function returns the following:\n",
    "    1. The key/value pair corresponding to the recall_macro score\n",
    "    2. The key/value pair corresponding to the acc_macro score\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    eval_dict = {key: eval_dict[key] for key in eval_dict.keys()\n",
    "                 & {'recall_macro', 'acc_macro'}}\n",
    "    for key in eval_dict.keys():\n",
    "        eval_dict[key] = round(eval_dict[key],4)\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a71fe6",
   "metadata": {},
   "source": [
    "## visualize_all_recall_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8001c2",
   "metadata": {},
   "source": [
    "Takes no arguments. Returns the recall_macro plots comparing the recall_macro scores of the train and validation\n",
    "sets for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6740347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_recall_plots():\n",
    "    \"\"\"\n",
    "    Takes no arguments. Returns the recall_macro plots comparing the recall_macro scores of the train and validation\n",
    "    sets for every model.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axs = plt.subplots(6, 3,sharex=False,sharey=False, figsize=(15, 20))\n",
    "\n",
    " \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.4)\n",
    "    \n",
    "\n",
    "    cnn_counter = 0\n",
    "    for i in range(0,6):  \n",
    "        for j in range(0,3):\n",
    "            if i+j == 0:\n",
    "                history = globals()['results_baseline'].history\n",
    "                axs[i, j].plot(history['recall_macro'])\n",
    "                axs[i, j].plot(history['val_recall_macro'])\n",
    "                axs[i, j].set_title(\"Baseline Model \")\n",
    "                cnn_counter+=1\n",
    "            else:\n",
    "                history = globals()['results_cnn' + str(cnn_counter)].history\n",
    "                axs[i, j].plot(history['recall_macro'])\n",
    "                axs[i, j].plot(history['val_recall_macro'])\n",
    "                axs[i, j].set_title(\"CNN \" + str(cnn_counter))\n",
    "                cnn_counter+=1\n",
    "            \n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Epochs', ylabel='Recall Scores') \n",
    "    fig.legend(['recall','val_recall'], fontsize=16);\n",
    "    fig.suptitle('Train Recall and Validation Recall', fontsize=16);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df932d9",
   "metadata": {},
   "source": [
    "## visualize_training_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5f431",
   "metadata": {},
   "source": [
    "Note: The code for this function was taken from a Flatiron School lab called 'Deeper Neural Networks - Lab'.\n",
    "\n",
    "This function takes as an as an argument the variable returned by fitting a tensorflow model. \n",
    "it is of type: tensorflow.python.keras.callbacks.History\n",
    "\n",
    "\n",
    "The best_model function returns the following plot:\n",
    "1. The change in macro_recall(y-axis), with respect to the number of epochs(x-axis),for both the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6d7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(results):\n",
    "    \"\"\"\n",
    "    This function takes as an as an argument the variable returned by fitting a tensorflow model. \n",
    "    it is of type: tensorflow.python.keras.callbacks.History\n",
    "\n",
    "\n",
    "    The best_model function returns the following plot:\n",
    "    1. The change in recall(y-axis) with respect to the number of epochs(x-axis)for both the train and validation sets\n",
    "    \"\"\"\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['recall_macro'])\n",
    "    plt.plot(history['val_recall_macro'])\n",
    "    plt.legend(['recall', 'val_recall'])\n",
    "    plt.title('Train Recall and Validation Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Train Recall and Val Recall')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672efb47",
   "metadata": {},
   "source": [
    "# Importing and Organizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821741b",
   "metadata": {},
   "source": [
    "I will:\n",
    "\n",
    "1. create a dataframe for each .csv file: training, validation, and testing. \n",
    "2. review all dataframes for: length, data types,missing data, etc.\n",
    "3. check for labels that 'occur' atleast 300 times in the trainng dataframe, and create new datframes for each data-      set with only those labels.\n",
    "4. Convert the new dataframes to numpy arrays.\n",
    "5. Create an 'image array' for each dataframe, and use a loop to add the images referred to in each dataframe to its      corresponding array,as a collection of pixels, with each pixel being represented by three values (0-255),\n",
    "   which represent the intensity of each respective color, red, green, and blue, for that pixel.\n",
    "6. Normalize all the pixel values by dividing each by 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd030fa5",
   "metadata": {},
   "source": [
    "<b>Creating a dataframe for each data set train, test, val:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f7548d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_original = pd.read_csv('archive/Training_Set/Training_Set/RFMiD_Training_Labels.csv')\n",
    "val_df_original = pd.read_csv('archive/Evaluation_Set/Evaluation_Set/RFMiD_Validation_Labels.csv')\n",
    "test_df_original = pd.read_csv('archive/Test_Set/Test_Set/RFMiD_Testing_Labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a2513",
   "metadata": {},
   "source": [
    "<b>Reviewing dataframe:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95002b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Disease_Risk</th>\n",
       "      <th>DR</th>\n",
       "      <th>ARMD</th>\n",
       "      <th>MH</th>\n",
       "      <th>DN</th>\n",
       "      <th>MYA</th>\n",
       "      <th>BRVO</th>\n",
       "      <th>TSLN</th>\n",
       "      <th>ERM</th>\n",
       "      <th>LS</th>\n",
       "      <th>MS</th>\n",
       "      <th>CSR</th>\n",
       "      <th>ODC</th>\n",
       "      <th>CRVO</th>\n",
       "      <th>TV</th>\n",
       "      <th>AH</th>\n",
       "      <th>ODP</th>\n",
       "      <th>ODE</th>\n",
       "      <th>ST</th>\n",
       "      <th>AION</th>\n",
       "      <th>PT</th>\n",
       "      <th>RT</th>\n",
       "      <th>RS</th>\n",
       "      <th>CRS</th>\n",
       "      <th>EDN</th>\n",
       "      <th>RPEC</th>\n",
       "      <th>MHL</th>\n",
       "      <th>RP</th>\n",
       "      <th>CWS</th>\n",
       "      <th>CB</th>\n",
       "      <th>ODPM</th>\n",
       "      <th>PRH</th>\n",
       "      <th>MNF</th>\n",
       "      <th>HR</th>\n",
       "      <th>CRAO</th>\n",
       "      <th>TD</th>\n",
       "      <th>CME</th>\n",
       "      <th>PTCR</th>\n",
       "      <th>CF</th>\n",
       "      <th>VH</th>\n",
       "      <th>MCA</th>\n",
       "      <th>VS</th>\n",
       "      <th>BRAO</th>\n",
       "      <th>PLQ</th>\n",
       "      <th>HPED</th>\n",
       "      <th>CL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1917</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1918</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Disease_Risk  DR  ARMD  MH  DN  MYA  BRVO  TSLN  ERM  LS  MS  CSR  \\\n",
       "0        1             1   1     0   0   0    0     0     0    0   0   0    0   \n",
       "1        2             1   1     0   0   0    0     0     0    0   0   0    0   \n",
       "2        3             1   1     0   0   0    0     0     0    0   0   0    0   \n",
       "3        4             1   0     0   1   0    0     0     0    0   0   0    0   \n",
       "4        5             1   1     0   0   0    0     0     0    0   1   0    0   \n",
       "...    ...           ...  ..   ...  ..  ..  ...   ...   ...  ...  ..  ..  ...   \n",
       "1915  1916             1   0     0   0   0    0     0     0    0   0   0    0   \n",
       "1916  1917             1   1     0   0   0    0     0     0    0   1   0    0   \n",
       "1917  1918             0   0     0   0   0    0     0     0    0   0   0    0   \n",
       "1918  1919             0   0     0   0   0    0     0     0    0   0   0    0   \n",
       "1919  1920             0   0     0   0   0    0     0     0    0   0   0    0   \n",
       "\n",
       "      ODC  CRVO  TV  AH  ODP  ODE  ST  AION  PT  RT  RS  CRS  EDN  RPEC  MHL  \\\n",
       "0       0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "1       0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "2       0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "3       1     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "4       0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "...   ...   ...  ..  ..  ...  ...  ..   ...  ..  ..  ..  ...  ...   ...  ...   \n",
       "1915    0     0   0   0    1    0   0     0   0   0   0    0    0     0    0   \n",
       "1916    0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "1917    0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "1918    0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "1919    0     0   0   0    0    0   0     0   0   0   0    0    0     0    0   \n",
       "\n",
       "      RP  CWS  CB  ODPM  PRH  MNF  HR  CRAO  TD  CME  PTCR  CF  VH  MCA  VS  \\\n",
       "0      0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "1      0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "2      0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "3      0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "4      0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "...   ..  ...  ..   ...  ...  ...  ..   ...  ..  ...   ...  ..  ..  ...  ..   \n",
       "1915   0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "1916   0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "1917   0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "1918   0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "1919   0    0   0     0    0    0   0     0   0    0     0   0   0    0   0   \n",
       "\n",
       "      BRAO  PLQ  HPED  CL  \n",
       "0        0    0     0   0  \n",
       "1        0    0     0   0  \n",
       "2        0    0     0   0  \n",
       "3        0    0     0   0  \n",
       "4        0    0     0   0  \n",
       "...    ...  ...   ...  ..  \n",
       "1915     0    0     0   0  \n",
       "1916     0    0     0   0  \n",
       "1917     0    0     0   0  \n",
       "1918     0    0     0   0  \n",
       "1919     0    0     0   0  \n",
       "\n",
       "[1920 rows x 47 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train_df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa208d35",
   "metadata": {},
   "source": [
    "<b>Reviewing all three dataframes:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f87a5c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1920 entries, 0 to 1919\n",
      "Data columns (total 47 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   ID            1920 non-null   int64\n",
      " 1   Disease_Risk  1920 non-null   int64\n",
      " 2   DR            1920 non-null   int64\n",
      " 3   ARMD          1920 non-null   int64\n",
      " 4   MH            1920 non-null   int64\n",
      " 5   DN            1920 non-null   int64\n",
      " 6   MYA           1920 non-null   int64\n",
      " 7   BRVO          1920 non-null   int64\n",
      " 8   TSLN          1920 non-null   int64\n",
      " 9   ERM           1920 non-null   int64\n",
      " 10  LS            1920 non-null   int64\n",
      " 11  MS            1920 non-null   int64\n",
      " 12  CSR           1920 non-null   int64\n",
      " 13  ODC           1920 non-null   int64\n",
      " 14  CRVO          1920 non-null   int64\n",
      " 15  TV            1920 non-null   int64\n",
      " 16  AH            1920 non-null   int64\n",
      " 17  ODP           1920 non-null   int64\n",
      " 18  ODE           1920 non-null   int64\n",
      " 19  ST            1920 non-null   int64\n",
      " 20  AION          1920 non-null   int64\n",
      " 21  PT            1920 non-null   int64\n",
      " 22  RT            1920 non-null   int64\n",
      " 23  RS            1920 non-null   int64\n",
      " 24  CRS           1920 non-null   int64\n",
      " 25  EDN           1920 non-null   int64\n",
      " 26  RPEC          1920 non-null   int64\n",
      " 27  MHL           1920 non-null   int64\n",
      " 28  RP            1920 non-null   int64\n",
      " 29  CWS           1920 non-null   int64\n",
      " 30  CB            1920 non-null   int64\n",
      " 31  ODPM          1920 non-null   int64\n",
      " 32  PRH           1920 non-null   int64\n",
      " 33  MNF           1920 non-null   int64\n",
      " 34  HR            1920 non-null   int64\n",
      " 35  CRAO          1920 non-null   int64\n",
      " 36  TD            1920 non-null   int64\n",
      " 37  CME           1920 non-null   int64\n",
      " 38  PTCR          1920 non-null   int64\n",
      " 39  CF            1920 non-null   int64\n",
      " 40  VH            1920 non-null   int64\n",
      " 41  MCA           1920 non-null   int64\n",
      " 42  VS            1920 non-null   int64\n",
      " 43  BRAO          1920 non-null   int64\n",
      " 44  PLQ           1920 non-null   int64\n",
      " 45  HPED          1920 non-null   int64\n",
      " 46  CL            1920 non-null   int64\n",
      "dtypes: int64(47)\n",
      "memory usage: 705.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67537ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 640 entries, 0 to 639\n",
      "Data columns (total 47 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   ID            640 non-null    int64\n",
      " 1   Disease_Risk  640 non-null    int64\n",
      " 2   DR            640 non-null    int64\n",
      " 3   ARMD          640 non-null    int64\n",
      " 4   MH            640 non-null    int64\n",
      " 5   DN            640 non-null    int64\n",
      " 6   MYA           640 non-null    int64\n",
      " 7   BRVO          640 non-null    int64\n",
      " 8   TSLN          640 non-null    int64\n",
      " 9   ERM           640 non-null    int64\n",
      " 10  LS            640 non-null    int64\n",
      " 11  MS            640 non-null    int64\n",
      " 12  CSR           640 non-null    int64\n",
      " 13  ODC           640 non-null    int64\n",
      " 14  CRVO          640 non-null    int64\n",
      " 15  TV            640 non-null    int64\n",
      " 16  AH            640 non-null    int64\n",
      " 17  ODP           640 non-null    int64\n",
      " 18  ODE           640 non-null    int64\n",
      " 19  ST            640 non-null    int64\n",
      " 20  AION          640 non-null    int64\n",
      " 21  PT            640 non-null    int64\n",
      " 22  RT            640 non-null    int64\n",
      " 23  RS            640 non-null    int64\n",
      " 24  CRS           640 non-null    int64\n",
      " 25  EDN           640 non-null    int64\n",
      " 26  RPEC          640 non-null    int64\n",
      " 27  MHL           640 non-null    int64\n",
      " 28  RP            640 non-null    int64\n",
      " 29  CWS           640 non-null    int64\n",
      " 30  CB            640 non-null    int64\n",
      " 31  ODPM          640 non-null    int64\n",
      " 32  PRH           640 non-null    int64\n",
      " 33  MNF           640 non-null    int64\n",
      " 34  HR            640 non-null    int64\n",
      " 35  CRAO          640 non-null    int64\n",
      " 36  TD            640 non-null    int64\n",
      " 37  CME           640 non-null    int64\n",
      " 38  PTCR          640 non-null    int64\n",
      " 39  CF            640 non-null    int64\n",
      " 40  VH            640 non-null    int64\n",
      " 41  MCA           640 non-null    int64\n",
      " 42  VS            640 non-null    int64\n",
      " 43  BRAO          640 non-null    int64\n",
      " 44  PLQ           640 non-null    int64\n",
      " 45  HPED          640 non-null    int64\n",
      " 46  CL            640 non-null    int64\n",
      "dtypes: int64(47)\n",
      "memory usage: 235.1 KB\n"
     ]
    }
   ],
   "source": [
    "val_df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "878ed8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 640 entries, 0 to 639\n",
      "Data columns (total 47 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   ID            640 non-null    int64\n",
      " 1   Disease_Risk  640 non-null    int64\n",
      " 2   DR            640 non-null    int64\n",
      " 3   ARMD          640 non-null    int64\n",
      " 4   MH            640 non-null    int64\n",
      " 5   DN            640 non-null    int64\n",
      " 6   MYA           640 non-null    int64\n",
      " 7   BRVO          640 non-null    int64\n",
      " 8   TSLN          640 non-null    int64\n",
      " 9   ERM           640 non-null    int64\n",
      " 10  LS            640 non-null    int64\n",
      " 11  MS            640 non-null    int64\n",
      " 12  CSR           640 non-null    int64\n",
      " 13  ODC           640 non-null    int64\n",
      " 14  CRVO          640 non-null    int64\n",
      " 15  TV            640 non-null    int64\n",
      " 16  AH            640 non-null    int64\n",
      " 17  ODP           640 non-null    int64\n",
      " 18  ODE           640 non-null    int64\n",
      " 19  ST            640 non-null    int64\n",
      " 20  AION          640 non-null    int64\n",
      " 21  PT            640 non-null    int64\n",
      " 22  RT            640 non-null    int64\n",
      " 23  RS            640 non-null    int64\n",
      " 24  CRS           640 non-null    int64\n",
      " 25  EDN           640 non-null    int64\n",
      " 26  RPEC          640 non-null    int64\n",
      " 27  MHL           640 non-null    int64\n",
      " 28  RP            640 non-null    int64\n",
      " 29  CWS           640 non-null    int64\n",
      " 30  CB            640 non-null    int64\n",
      " 31  ODPM          640 non-null    int64\n",
      " 32  PRH           640 non-null    int64\n",
      " 33  MNF           640 non-null    int64\n",
      " 34  HR            640 non-null    int64\n",
      " 35  CRAO          640 non-null    int64\n",
      " 36  TD            640 non-null    int64\n",
      " 37  CME           640 non-null    int64\n",
      " 38  PTCR          640 non-null    int64\n",
      " 39  CF            640 non-null    int64\n",
      " 40  VH            640 non-null    int64\n",
      " 41  MCA           640 non-null    int64\n",
      " 42  VS            640 non-null    int64\n",
      " 43  BRAO          640 non-null    int64\n",
      " 44  PLQ           640 non-null    int64\n",
      " 45  HPED          640 non-null    int64\n",
      " 46  CL            640 non-null    int64\n",
      "dtypes: int64(47)\n",
      "memory usage: 235.1 KB\n"
     ]
    }
   ],
   "source": [
    "test_df_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325ceff",
   "metadata": {},
   "source": [
    "<b>They all have 47 columns, no missing data, and the data-type for all columns is int64</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001717a",
   "metadata": {},
   "source": [
    "<b>Looking for classes that 'occur' atleast 300 times in the traing dataframe:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a10db6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              1844160\n",
       "Disease_Risk       1519\n",
       "DR                  376\n",
       "MH                  317\n",
       "ODC                 282\n",
       "TSLN                186\n",
       "DN                  138\n",
       "MYA                 101\n",
       "ARMD                100\n",
       "BRVO                 73\n",
       "ODP                  65\n",
       "ODE                  58\n",
       "LS                   47\n",
       "RS                   43\n",
       "CSR                  37\n",
       "CRS                  32\n",
       "CRVO                 28\n",
       "RPEC                 22\n",
       "AION                 17\n",
       "AH                   16\n",
       "EDN                  15\n",
       "MS                   15\n",
       "RT                   14\n",
       "ERM                  14\n",
       "MHL                  11\n",
       "PT                   11\n",
       "RP                    6\n",
       "TV                    6\n",
       "ST                    5\n",
       "PTCR                  5\n",
       "CME                   4\n",
       "CF                    3\n",
       "TD                    3\n",
       "MNF                   3\n",
       "CWS                   3\n",
       "BRAO                  2\n",
       "PRH                   2\n",
       "CRAO                  2\n",
       "HPED                  1\n",
       "VH                    1\n",
       "MCA                   1\n",
       "VS                    1\n",
       "CB                    1\n",
       "PLQ                   1\n",
       "CL                    1\n",
       "ODPM                  0\n",
       "HR                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_original.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74264869",
   "metadata": {},
   "source": [
    "<b>There are only three: 'Disease_Risk','DR',and 'MH'. I will create new dataframes with only these columns:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beae4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_original[['Disease_Risk','DR','MH']]\n",
    "val_df = val_df_original[['Disease_Risk','DR','MH']]\n",
    "test_df = test_df_original[['Disease_Risk','DR','MH']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080357bd",
   "metadata": {},
   "source": [
    "<b>Reviewing new dataframes:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79232d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1920 entries, 0 to 1919\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   Disease_Risk  1920 non-null   int64\n",
      " 1   DR            1920 non-null   int64\n",
      " 2   MH            1920 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 45.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dcad81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease_Risk</th>\n",
       "      <th>DR</th>\n",
       "      <th>MH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Disease_Risk  DR  MH\n",
       "0             1   1   0\n",
       "1             1   1   0\n",
       "2             1   1   0\n",
       "3             1   0   1\n",
       "4             1   1   0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4923f3c",
   "metadata": {},
   "source": [
    "<b>Converting new dataframes to numpy arrays:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ca74073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels shape:  (1920, 3)\n",
      "val_labels shape:  (640, 3)\n",
      "test_labels shape:  (640, 3)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_df.to_numpy()\n",
    "val_labels = val_df.to_numpy()\n",
    "test_labels = test_df.to_numpy()\n",
    "\n",
    "print('train_labels shape: ', train_labels.shape)\n",
    "print('val_labels shape: ', val_labels.shape)\n",
    "print('test_labels shape: ', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af3f71",
   "metadata": {},
   "source": [
    "<b>Reviewing new numpy arrays:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48585fb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels shape:  (1920, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('train_labels shape: ', train_labels.shape)\n",
    "train_labels[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5904ed",
   "metadata": {},
   "source": [
    "<b>Creating an array of images for each corresponding dataframe:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d82367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1620/1920 [01:22<00:55,  5.41it/s]"
     ]
    }
   ],
   "source": [
    "# creating array of train set images\n",
    "train_image = []\n",
    "for i in tqdm(range(1,1921)):\n",
    "    img = image.load_img('archive/Training_Set/Training_Set/Training/'+str(i)+'.png',target_size=(128,128,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "train_images = np.array(train_image)\n",
    "\n",
    "# creating array of validation set images\n",
    "val_image = []\n",
    "for i in tqdm(range(1,641)):\n",
    "    img = image.load_img('archive/Evaluation_Set/Evaluation_Set/Validation/'+str(i)+'.png',target_size=(128,128,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    val_image.append(img)\n",
    "val_images = np.array(val_image)\n",
    "\n",
    "# creating array of test set images\n",
    "test_image = []\n",
    "for i in tqdm(range(1,641)):\n",
    "    img = image.load_img('archive/Test_Set/Test_Set/Test/'+str(i)+'.png',target_size=(128,128,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    test_image.append(img)\n",
    "test_images = np.array(test_image)\n",
    "\n",
    "print('train_images shape: ', train_images.shape)\n",
    "print('val_images shape: ', val_images.shape)\n",
    "print('test_images shape: ', test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c75f8",
   "metadata": {},
   "source": [
    "<b>Creating three new datasets, one each for 'train', 'test', and 'val' images, where the arrays are unrowed. This is required for use with the multi-layer perceptron:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "train_img_unrow_dataset = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img_unrow_dataset = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img_unrow_dataset = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img_unrow_dataset.shape)\n",
    "print(test_img_unrow_dataset.shape)\n",
    "print(val_img_unrow_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67310324",
   "metadata": {},
   "source": [
    "## Checking If Data Needs to be Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize pixel values\n",
    "print('Train images min value:', train_images.min(),'Train images max value:', train_images.max())\n",
    "print('Validation images min value:', val_images.min(),'Validation images max value:', val_images.max())\n",
    "print('Test images min value:', test_images.min(),'Test images max value:', test_images.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a336d3",
   "metadata": {},
   "source": [
    "<b>The images are confirmed normalized.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60db748",
   "metadata": {},
   "source": [
    "## Viewing an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb108867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an image of 'healthy' retina\n",
    "normal_img = image.load_img('archive/Training_Set/Training_Set/Training/10.png',target_size=(128,192,3))\n",
    "print('normal image')\n",
    "normal_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an image of 'healthy' retina\n",
    "dr_img = image.load_img('archive/Training_Set/Training_Set/Training/1.png',target_size=(128,192,3))\n",
    "print('diabetic retinopathy image')\n",
    "dr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9fda4",
   "metadata": {},
   "source": [
    "<b>'The image is labeled as DR if it shows any of the following clinical findings: microaneurysms, retinal dot and blot hemorrhage, hard exudates or cotton wool spots' **(see Figure 2a) [17]</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60468b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an image of 'healthy' retina\n",
    "mh_img = image.load_img('archive/Training_Set/Training_Set/Training/780.png',target_size=(128,192,3))\n",
    "print('media haze image')\n",
    "mh_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff32935",
   "metadata": {},
   "source": [
    "<b>'The opacity of media can be a hallmark for the presence of cataracts' **.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51472a",
   "metadata": {},
   "source": [
    "<b>Reference(s):\n",
    "** Pachade, S.; Porwal, P.; Thulkar, D.; Kokare, M.; Deshmukh, G.; Sahasrabuddhe, V.; Giancardo, L.; Quellec, G.;         Mériaudeau, F. Retinal Fundus Multi-Disease Image Dataset (RFMiD): A Dataset for Multi-Disease Detection Research.     Data 2021, 6, 14. https://doi.org/10.3390/data6020014</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197551a",
   "metadata": {},
   "source": [
    "# Neural Network models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327245c",
   "metadata": {},
   "source": [
    "## Building a baseline model and verifying the custom metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3240a7",
   "metadata": {},
   "source": [
    "In this section I will:\n",
    "1. Create a multi-layer perceptron as a baseline model\n",
    "2. Use this model to comnfirm that the custom metrics work as they are supposed to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de281b4",
   "metadata": {},
   "source": [
    "### Building a multi-layer perceptron as a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "# Build a baseline fully connected model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "baseline_model = models.Sequential()\n",
    "baseline_model.add(layers.Dense(64, activation='relu', input_shape=(49152,))) \n",
    "baseline_model.add(layers.Dense(32, activation='relu'))\n",
    "baseline_model.add(layers.Dense(16, activation='relu'))\n",
    "baseline_model.add(layers.Dense(3, activation='sigmoid'))\n",
    "baseline_model.compile(optimizer='sgd',\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics= [recall_macro,acc_macro]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline =  baseline_model.fit(train_img_unrow_dataset,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_img_unrow_dataset,val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43699789",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc5a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_train_eval_dict = baseline_model.evaluate(train_img_unrow_dataset, train_labels, return_dict=1, verbose=0)\n",
    "recall_dict(baseline_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_val_eval_dict = baseline_model.evaluate(val_img_unrow_dataset, val_labels,return_dict=1, verbose=0)\n",
    "recall_dict(baseline_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test_eval_dict = baseline_model.evaluate(test_img_unrow_dataset, test_labels,return_dict=1, verbose=0)\n",
    "recall_dict(baseline_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d85a2a",
   "metadata": {},
   "source": [
    "<b>Going forward, I will use these as my 'baseline' scores.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110b20d",
   "metadata": {},
   "source": [
    "### Verifying Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb1c12",
   "metadata": {},
   "source": [
    "In this section:\n",
    "\n",
    "1. The custom metric acc_macro should return the average of the indvidual accuracy scores of each of the three classes. I will use training data predictions to confirm that the custom metric works as expected.\n",
    "\n",
    "2. The custom metric recall_macro should return the average of the indvidual recall scores of each of the three classes I will use training data predictions to confirm that the custom metric recall_macro works as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa2d3a",
   "metadata": {},
   "source": [
    "<b>Verifying acc_macro:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating numpy array of training set predictions\n",
    "b_line_train_preds = baseline_model.predict(train_img_unrow_dataset)\n",
    "b_line_train_preds = np.round(b_line_train_preds)\n",
    "# making sure that both 'b_line_train_preds' and 'b_line_train_labels' are type float\n",
    "b_line_train_preds_II = b_line_train_preds.astype(float)\n",
    "train_labels_II = train_labels.astype(float)\n",
    "\n",
    "# didviding both the 'train_preds' and 'train_labels' arrays into three parts. One for each class.\n",
    "b_line_train_preds_II_0 = b_line_train_preds_II[:,0]\n",
    "train_labels_II_0 = train_labels_II[:,0]\n",
    "\n",
    "b_line_train_preds_II_1 = b_line_train_preds_II[:,1]\n",
    "train_labels_II_1 = train_labels_II[:,1]\n",
    "\n",
    "b_line_train_preds_II_2 = b_line_train_preds_II[:,2]\n",
    "train_labels_II_2 = train_labels_II[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37e296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the Keras 'Accuracy' metric to calculate the acuracy for each respective class.\n",
    "\n",
    "# Keras accuracy for class 0:\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(train_labels_II_0, b_line_train_preds_II_0)\n",
    "accuracy_score_class_0 = m.result().numpy()\n",
    "\n",
    "# Keras accuracy for class 1:\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(train_labels_II_1, b_line_train_preds_II_1)\n",
    "accuracy_score_class_1 = m.result().numpy()\n",
    "\n",
    "# Keras accuracy for class 2:\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state(train_labels_II_2, b_line_train_preds_II_2)\n",
    "accuracy_score_class_2 = m.result().numpy()\n",
    "\n",
    "# 'accuracy_macro' is the average of the three Keras 'Accuracy' scores:\n",
    "accuracy_macro = (accuracy_score_class_0 + accuracy_score_class_1 + accuracy_score_class_2)/3\n",
    "\n",
    "\n",
    "\n",
    "# Using the custom metric 'acc' to calculate the acuracy for each respective class:\n",
    "\n",
    "acc_score_class_0 = acc(train_labels_II_0, b_line_train_preds_II_0).numpy()\n",
    "acc_score_class_1 = acc(train_labels_II_1, b_line_train_preds_II_1).numpy()\n",
    "acc_score_class_2 = acc(train_labels_II_2, b_line_train_preds_II_2).numpy()\n",
    "\n",
    "# 'acc_macro_score' is the average of the three 'acc' scores using the custom metric 'acc_macro':\n",
    "acc_macro_score = acc_macro(train_labels_II, b_line_train_preds_II).numpy()\n",
    "\n",
    "print('Results using the Keras \"Accuracy\" metric:')\n",
    "print('accuracy_score_class_0:', accuracy_score_class_0)\n",
    "print('accuracy_score_class_1:', accuracy_score_class_1)\n",
    "print('accuracy_score_class_2:', accuracy_score_class_2)\n",
    "print('accuracy_macro score: ',  accuracy_macro,'\\n\\n')\n",
    "\n",
    "print('Results using the custom \"acc\", and \"acc_macro\" metric:')\n",
    "print('acc_score_class_0:', acc_score_class_0)\n",
    "print('acc_score_class_1:', acc_score_class_1)\n",
    "print('acc_score_class_2:', acc_score_class_2)\n",
    "print('acc_macro_score: ',  acc_macro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130fb8d9",
   "metadata": {},
   "source": [
    "<b>The results are identical to five decimal points or more</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74be85",
   "metadata": {},
   "source": [
    "<b>Verifying recall_macro:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932b499",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# producing sklearn's classification report to view their recall macro avg:\n",
    "class_report = (classification_report(train_labels_II, b_line_train_preds_II))\n",
    "\n",
    "# the macro average of the recall scores of each respective class, returned by the custom metric \"recall_macro\":\n",
    "recall_macro_score = np.round(recall_macro(train_labels_II, b_line_train_preds_II).numpy(),2)\n",
    "\n",
    "print(\"sklearn's classification report:\\n\\n\",class_report)\n",
    "print('recall_macro_score: ',recall_macro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc7d1c",
   "metadata": {},
   "source": [
    "<b>The macro avaerage of the recall score produced by the classification report, and the result returned by the custom metric \"recall_macro\" are identical.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f9fda",
   "metadata": {},
   "source": [
    " ## Build a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e03a4",
   "metadata": {},
   "source": [
    "1. I will try to overfit while also checking to see if that rise in the training score had any impact on the validation score.<br><br>\n",
    "2. From there, when I am satisfied with the model architecture(number of layers, and number of nodes per layer), I  will try to reduce the variance by using different combinations of filter sizes, with the goal of making the model more generalizable.<br><br>\n",
    "3. Once I am satisifed with the filters, I would apply an optimizer with different learning rates, to see if this would improve results, or give me additional information about where the line plots for the scores of the respective data sets stop oscillating, and possibly reach their respective convergences'.<br><br>\n",
    "4. After this, I would choose the best model amongst these and try to further reduce the variance by using some regularization technique, such as adding a dropout layer(s), or using L1, or L2 regularization, or poassiblly combining techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61dc671",
   "metadata": {},
   "source": [
    "## CNN-1 through CNN-5: Try to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dabaff",
   "metadata": {},
   "source": [
    " ### CNN-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2007d",
   "metadata": {},
   "source": [
    "<b>The goal in the first few models is to get as high a training score as possible, while also looking at how this impacts the validation score. I have my baseline scores as:\n",
    "\n",
    "training :   0.6611<br>\n",
    "validation:  0.5189</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_1 = models.Sequential()\n",
    "cnn_model_1.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Flatten())\n",
    "cnn_model_1.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_1.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_1.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa854105",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn1 = cnn_model_1.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbc031",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc34294",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1_train_eval_dict = cnn_model_1.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_1_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c059be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1_val_eval_dict = cnn_model_1.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_1_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1f7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_1_test_eval_dict = cnn_model_1.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_1_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7d2fd",
   "metadata": {},
   "source": [
    "<b>The initial CNN traing score was slightly lower than that of the baseline model, but the validation score is higher, which is a good thing.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc283a0",
   "metadata": {},
   "source": [
    " ### CNN-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac635b90",
   "metadata": {},
   "source": [
    "<b>I will double the number of nodes in the input layer of the CNN-1 model, to see if I could raise the training score:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_2 = models.Sequential()\n",
    "cnn_model_2.add(layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Flatten())\n",
    "cnn_model_2.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_2.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_2.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5535dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn2 = cnn_model_2.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2_train_eval_dict = cnn_model_2.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_2_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4631b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2_val_eval_dict = cnn_model_2.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_2_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161eb938",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2_test_eval_dict = cnn_model_2.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_2_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c910f5",
   "metadata": {},
   "source": [
    "<b>Both the traing and validation scores have gone down.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21efd9fa",
   "metadata": {},
   "source": [
    " ### CNN-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0de7c7",
   "metadata": {},
   "source": [
    "<b>Given that the score actually went down with CNN-2, I will keep the architecture the same as CNN-1, but add an 8 node layer to the beginning of the model(input layer):</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_3 = models.Sequential()\n",
    "cnn_model_3.add(layers.Conv2D(8, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Conv2D(16, (2, 2), activation='relu'))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Flatten())\n",
    "cnn_model_3.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_3.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab063c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn3 = cnn_model_3.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48649e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_train_eval_dict = cnn_model_3.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_3_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_val_eval_dict = cnn_model_3.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_3_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b38770",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_test_eval_dict = cnn_model_3.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_3_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0145eb5",
   "metadata": {},
   "source": [
    "<b>Both the training and validation scores are considerably lower than CNN-1.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf319b59",
   "metadata": {},
   "source": [
    " ### CNN-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c2e80",
   "metadata": {},
   "source": [
    "<b>I will keep the same architecture as CNN-1 but add padding, so that the model can potentially gain more information about the respective edges at each layer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9edac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_4 = models.Sequential()\n",
    "cnn_model_4.add(layers.Conv2D(16, (2, 2),padding='same',activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Conv2D(32, (2, 2),padding='same', activation='relu'))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Conv2D(64, (2, 2),padding='same', activation='relu'))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Conv2D(128, (2, 2),padding='same', activation='relu'))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Flatten())\n",
    "cnn_model_4.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_4.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_4.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d06402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn4 = cnn_model_4.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_4_train_eval_dict = cnn_model_4.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_4_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_4_val_eval_dict = cnn_model_4.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_4_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e294b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_4_test_eval_dict = cnn_model_4.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_4_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533dffb1",
   "metadata": {},
   "source": [
    "<b>The padding slightly increased the training score, but lowered the test score (relative to CNN-1), thereby increasing variance. For this reason, moving forward, I will not use the padding.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349bbc5",
   "metadata": {},
   "source": [
    " ### CNN-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8141b6",
   "metadata": {},
   "source": [
    "<b>I will modify CNN-1 by removing the last pooling layer, so that the classification portion of the model has more data to work with.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db83436",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_5 = models.Sequential()\n",
    "cnn_model_5.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_5.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_5.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "cnn_model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_5.add(layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "\n",
    "cnn_model_5.add(layers.Flatten())\n",
    "cnn_model_5.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_5.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_5.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn5 = cnn_model_5.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd46045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_5_train_eval_dict = cnn_model_5.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_5_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69aa2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_5_val_eval_dict = cnn_model_5.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_5_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_5_test_eval_dict = cnn_model_5.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_5_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146fbda8",
   "metadata": {},
   "source": [
    "<b>Both scores have gone up. The train score has increased by almost four points, and the test score by close to three points. I feel this is a considerable increase in the test score, and that the possible increase in variance could be reconciled. For that reason, moving forward I will use this architecture.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429147b",
   "metadata": {},
   "source": [
    "## CNN-6 through CNN-9: Try different filter sizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c84364",
   "metadata": {},
   "source": [
    " ### CNN-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7115ec6",
   "metadata": {},
   "source": [
    "<b>I will use the same architetcture as in CNN-5, but add 3x3 filters on the last two activation layers to try to make the model more generalizeable, thereby reducing variance.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7172e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_6 = models.Sequential()\n",
    "cnn_model_6.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "cnn_model_6.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_6.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "cnn_model_6.add(layers.Flatten())\n",
    "cnn_model_6.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_6.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_6.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn6 = cnn_model_6.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_6_train_eval_dict = cnn_model_6.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_6_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_6_val_eval_dict = cnn_model_6.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_6_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa324ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_6_test_eval_dict = cnn_model_6.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_6_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264f56f",
   "metadata": {},
   "source": [
    "<b>Relative to CNN-5 the training score went up, but the test score went down, increasing variance.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8554fc7",
   "metadata": {},
   "source": [
    " ### CNN-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e5025",
   "metadata": {},
   "source": [
    "<b>I will use the same architetcture as in CNN-5, but will use a 3x3 filter, and a 5x5 filter on the last two activation layers respectively, to try to make the model more generalizeable, thereby reducing variance.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb930ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_7 = models.Sequential()\n",
    "cnn_model_7.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "cnn_model_7.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_7.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_7.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "\n",
    "cnn_model_7.add(layers.Flatten())\n",
    "cnn_model_7.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_7.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_7.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed84af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn7 = cnn_model_7.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8093d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_7_train_eval_dict = cnn_model_7.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_7_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e81b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_7_val_eval_dict = cnn_model_7.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_7_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3513870",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_7_test_eval_dict = cnn_model_7.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_7_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9344ac",
   "metadata": {},
   "source": [
    "<b>Both the training and test scores are lower than CNN-5, but the difference between the train and the test scores is virtually identical. For that reason, I will continue with the CNN-5 architecture going forward.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542d27a",
   "metadata": {},
   "source": [
    " ### CNN-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b5175",
   "metadata": {},
   "source": [
    "<b>I will use the same architetcture as in CNN-5, but will use a 3x3, 3x3, and 5x5 filter on the last three layers respectively,to try to make the model more generalizeable, thereby reducing variance.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_8 = models.Sequential()\n",
    "cnn_model_8.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_8.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn_model_8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_8.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_8.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "\n",
    "cnn_model_8.add(layers.Flatten())\n",
    "cnn_model_8.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_8.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "\n",
    "cnn_model_8.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn8 = cnn_model_8.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb38a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_8_train_eval_dict = cnn_model_8.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_8_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_8_val_eval_dict = cnn_model_8.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_8_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_8_test_eval_dict = cnn_model_8.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_8_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0c995",
   "metadata": {},
   "source": [
    "<b>These are the best scores yet. The training score is over two points less than CNN-5, yet the test score is only a quarter of a point less, thereby reducing the variance, while keeping the test score virtually the same. Going forward, these would be the scores to beat.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a97e6",
   "metadata": {},
   "source": [
    "### CNN-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4753a62",
   "metadata": {},
   "source": [
    "<b>I will use a 3x3, 3x3, 3x3, and 5x5 filter on the four layers respectively,to try to make the model more generalizeable, thereby reducing variance.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_9 = models.Sequential()\n",
    "cnn_model_9.add(layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "\n",
    "cnn_model_9.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn_model_9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_9.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_9.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "\n",
    "cnn_model_9.add(layers.Flatten())\n",
    "cnn_model_9.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_9.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_9.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn9 = cnn_model_9.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643042bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_9_train_eval_dict = cnn_model_9.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_9_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de065caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_9_val_eval_dict = cnn_model_9.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_9_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_9_test_eval_dict = cnn_model_9.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_9_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c149b8",
   "metadata": {},
   "source": [
    "<b>Interestingly enough this actually caused the training score to go up almost thirteen points, relative to CNN-8, but made the test score go down; making for a considerably worse variance. Going forward I will use the CNN-8 model architecture.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d45a9",
   "metadata": {},
   "source": [
    "## CNN-10 through CNN-11: Try different optimizers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5251aa",
   "metadata": {},
   "source": [
    "### CNN-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d1b7d",
   "metadata": {},
   "source": [
    "<b>I will use the same architecture as in CNN-8, but will apply the Adam optimizer, using all its default settings, to 'speed up' the learning process, and see how the model would progress.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63772ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_10 = models.Sequential()\n",
    "cnn_model_10.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_10.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "cnn_model_10.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn_model_10.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_10.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_10.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_10.add(layers.Conv2D(128, (5, 5),activation='relu'))\n",
    "\n",
    "cnn_model_10.add(layers.Flatten())\n",
    "cnn_model_10.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_10.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "\n",
    "cnn_model_10.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75efef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn10 = cnn_model_10.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7feb001",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_10_train_eval_dict = cnn_model_10.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_10_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_10_val_eval_dict = cnn_model_10.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_10_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_10_test_eval_dict = cnn_model_10.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_10_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3c10c",
   "metadata": {},
   "source": [
    "<b>The model has severely overfit, but it does give some interesting information. No matter how high the training score gets, the validation score appears to be converging at around .5. That said I will try one more model with the same Adam optimizer, but with a much smaller learning rate, to see how that goes.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c18afc",
   "metadata": {},
   "source": [
    "### CNN-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9bbf1",
   "metadata": {},
   "source": [
    "<b>I will run the same model as CNN-10 except I will decrease the default learning rate for the Adam optimizer to .00005</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642dede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "optim = tf.keras.optimizers.Adam(learning_rate = .00005)\n",
    "\n",
    "cnn_model_11 = models.Sequential()\n",
    "cnn_model_11.add(layers.Conv2D(16, (2, 2),activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_11.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn_model_11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_11.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_11.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "\n",
    "cnn_model_11.add(layers.Flatten())\n",
    "cnn_model_11.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_11.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_11.compile(loss='binary_crossentropy',\n",
    "              optimizer= optim,\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn11 = cnn_model_11.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa41948",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_11_train_eval_dict = cnn_model_11.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_11_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_11_val_eval_dict = cnn_model_11.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_11_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ce143",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_11_test_eval_dict = cnn_model_11.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_11_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ad93f",
   "metadata": {},
   "source": [
    "<b>The decrease in the learning rate definitely helped, but CNN-8 is still the best model. Going forward, I will continue with the model CNN-8 architecture, but use some regularization techniques to see if I could further reduce the variance.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed32fb7",
   "metadata": {},
   "source": [
    "## CNN-12 through CNN-17: Try different regularization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c067d9b2",
   "metadata": {},
   "source": [
    "### CNN-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adbac61",
   "metadata": {},
   "source": [
    "<b>I will run the same model architecture as in CNN-8, but with a dropout layer of .1, applied to all layers.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06821937",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_12 = models.Sequential()\n",
    "cnn_model_12.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_12.add(layers.Dropout(0.1))\n",
    "cnn_model_12.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_12.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn_model_12.add(layers.Dropout(0.1))\n",
    "cnn_model_12.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_12.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_12.add(layers.Dropout(0.1))\n",
    "cnn_model_12.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_12.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "cnn_model_12.add(layers.Dropout(0.1))\n",
    "\n",
    "cnn_model_12.add(layers.Flatten())\n",
    "cnn_model_12.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_12.add(layers.Dropout(0.1))\n",
    "cnn_model_12.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "\n",
    "cnn_model_12.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424967c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn12 = cnn_model_12.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c021288",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_12_train_eval_dict = cnn_model_12.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_12_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_12_val_eval_dict = cnn_model_12.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_12_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b511290",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_12_test_eval_dict = cnn_model_12.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_12_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27196c",
   "metadata": {},
   "source": [
    "<b>This actually increased the training score by over 1 point, as compared to CNN-8, while keeping the test score virtually the same; making the variance worse.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72eccd8",
   "metadata": {},
   "source": [
    "### CNN-13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03419d",
   "metadata": {},
   "source": [
    "<b>I will run the same model as CNN-12 but with dropout layers of .2 instead of .1 .</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_13 = models.Sequential()\n",
    "cnn_model_13.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_13.add(layers.Dropout(0.2))\n",
    "cnn_model_13.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_13.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn_model_13.add(layers.Dropout(0.2))\n",
    "cnn_model_13.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_13.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_13.add(layers.Dropout(0.2))\n",
    "cnn_model_13.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_13.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "cnn_model_13.add(layers.Dropout(0.2))\n",
    "\n",
    "cnn_model_13.add(layers.Flatten())\n",
    "cnn_model_13.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_13.add(layers.Dropout(0.2))\n",
    "cnn_model_13.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_13.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn13 = cnn_model_13.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde00bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66004856",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_13_train_eval_dict = cnn_model_13.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_13_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc539bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_13_val_eval_dict = cnn_model_13.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_13_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a16c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_13_test_eval_dict = cnn_model_13.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_13_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d7065",
   "metadata": {},
   "source": [
    "<b>The variance is even worse than CNN-12, which is not as good as CNN-8.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c71d1",
   "metadata": {},
   "source": [
    "### CNN-14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c82e00",
   "metadata": {},
   "source": [
    "<b>I will run the same model as CNN-13 but with dropout layers of .3 instead of .2 .</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdf1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_14 = models.Sequential()\n",
    "cnn_model_14.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_14.add(layers.Dropout(0.3))\n",
    "cnn_model_14.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_14.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn_model_14.add(layers.Dropout(0.3))\n",
    "cnn_model_14.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_14.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model_14.add(layers.Dropout(0.3))\n",
    "cnn_model_14.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_14.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "cnn_model_14.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "cnn_model_14.add(layers.Flatten())\n",
    "cnn_model_14.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_14.add(layers.Dropout(0.3))\n",
    "cnn_model_14.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_14.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d25a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn14 = cnn_model_14.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_14_plot = visualize_training_results(results_cnn14)\n",
    "cnn_14_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8061d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_14_train_eval_dict = cnn_model_14.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_14_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_14_val_eval_dict = cnn_model_14.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_14_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ade4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_14_test_eval_dict = cnn_model_14.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_14_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71297d04",
   "metadata": {},
   "source": [
    "<b>Compared to CNN-8, the train score has decreased by almost five points, while the test score has only decreased by about half a point. This marks a considerable improvement (decrease) in the variance.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2caed8a",
   "metadata": {},
   "source": [
    "### CNN-15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b2327",
   "metadata": {},
   "source": [
    "<b>I will use the same architecture as that of model CNN-14, but instead of using dropout layers, I will use L2(.01) regularization for each layer.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f02779",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_15 = models.Sequential()\n",
    "cnn_model_15.add(layers.Conv2D(16, (2, 2), kernel_regularizer=regularizers.l2(0.01), activation='relu',\n",
    "                        input_shape=(128,128,  3)))\n",
    "cnn_model_15.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_15.add(layers.Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "cnn_model_15.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_15.add(layers.Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "cnn_model_15.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_15.add(layers.Conv2D(128, (5, 5), kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "\n",
    "cnn_model_15.add(layers.Flatten())\n",
    "cnn_model_15.add(layers.Dense(32, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "cnn_model_15.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_15.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e26f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn15 = cnn_model_15.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff97290",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_15_plot = visualize_training_results(results_cnn15)\n",
    "cnn_15_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_15_train_eval_dict = cnn_model_15.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_15_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_15_val_eval_dict = cnn_model_15.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_15_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34396311",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_15_test_eval_dict = cnn_model_15.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_15_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f6c84",
   "metadata": {},
   "source": [
    "<b>Interesting results. Both scores went down considerably, relative to CNN-14. The train score, by almost fourteen points, and the test score by almost six. This definitely improves the variance, but I am not sure at what cost to the bias. I will review all the models at the end, to determine which is best.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed237f59",
   "metadata": {},
   "source": [
    "### CNN-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd1a4f",
   "metadata": {},
   "source": [
    "<b>I will use the same architecture as that of model CNN-15, but I will decrease the L2 regularization penalty to .0075 for each layer.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8afbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_16 = models.Sequential()\n",
    "cnn_model_16.add(layers.Conv2D(16, (2, 2),kernel_regularizer=regularizers.l2(0.0075) ,activation='relu',\n",
    "                        input_shape=(128,128,  3)))\n",
    "cnn_model_16.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_16.add(layers.Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.0075) , activation='relu'))\n",
    "cnn_model_16.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_16.add(layers.Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.0075), activation='relu'))\n",
    "cnn_model_16.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_16.add(layers.Conv2D(128, (5, 5), kernel_regularizer=regularizers.l2(0.0075), activation='relu'))\n",
    "\n",
    "cnn_model_16.add(layers.Flatten())\n",
    "cnn_model_16.add(layers.Dense(32, kernel_regularizer=regularizers.l2(0.0075), activation='relu'))\n",
    "cnn_model_16.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_16.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn16 = cnn_model_16.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da623c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_16_plot = visualize_training_results(results_cnn16)\n",
    "cnn_16_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8215d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_16_train_eval_dict = cnn_model_16.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_16_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_16_val_eval_dict = cnn_model_16.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_16_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e363a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_16_test_eval_dict = cnn_model_16.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_16_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7d07b",
   "metadata": {},
   "source": [
    "<b>Relative to CNN-15, the train score has increased about three points. The test score has also increased almost one point, but the variance has increased as well.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee60887",
   "metadata": {},
   "source": [
    "### CNN-17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc976b",
   "metadata": {},
   "source": [
    "<b>I will use the same architecture as that of model CNN-16, but I will decrease the L2 regularization penalty to .0025 for each layer.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9aca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_17 = models.Sequential()\n",
    "cnn_model_17.add(layers.Conv2D(16, (2, 2), kernel_regularizer=regularizers.l2(0.0025),activation='relu',\n",
    "                        input_shape=(128 ,128,  3)))\n",
    "cnn_model_17.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_17.add(layers.Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.0025), activation='relu'))\n",
    "cnn_model_17.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_17.add(layers.Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.0025), activation='relu'))\n",
    "cnn_model_17.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_17.add(layers.Conv2D(128, (5, 5), kernel_regularizer=regularizers.l2(0.0025), activation='relu'))\n",
    "\n",
    "cnn_model_17.add(layers.Flatten())\n",
    "cnn_model_17.add(layers.Dense(32, kernel_regularizer=regularizers.l2(0.0025), activation='relu'))\n",
    "cnn_model_17.add(layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "cnn_model_17.compile(loss='binary_crossentropy',\n",
    "              optimizer= 'sgd',\n",
    "              metrics=[recall_macro,acc_macro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9527263",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn17 = cnn_model_17.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=75,\n",
    "                    batch_size=16,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_17_plot = visualize_training_results(results_cnn17)\n",
    "cnn_17_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d176c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_17_train_eval_dict = cnn_model_17.evaluate(train_images, train_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_17_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_17_val_eval_dict = cnn_model_17.evaluate(val_images, val_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_17_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d595944",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_17_test_eval_dict = cnn_model_17.evaluate(test_images, test_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_17_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ae4d4",
   "metadata": {},
   "source": [
    "<b>Both the train and test scores have increased, as compared to CNN-16. This will be my final model. I will review all models in the next section to determine which is best.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297b4c3",
   "metadata": {},
   "source": [
    "#  Choosing The Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b293285",
   "metadata": {},
   "source": [
    "<b>After reviewing the 'Best Model' dataframe, and the train and recall validation plots below, there are a few major takeaways:<br>\n",
    "\n",
    "* The model with the best train/test - bias/variance combination is model cnn-15, based on auc score. \n",
    "* Independent of the train score, after around epoch 40 the majority of scores fall between the low 40s and the low     50s, and there is an upward trend across all validation plots. \n",
    "* In cnn-10 using the ADAM optimizer with the default learning rate; with the training score at 96%, the validation     score appears to be converging at around .50.\n",
    "* Not only does model cnn-15 have the highest auc score, it also has the lowest 'train-val-test diff mean', which is     derived when adding the train-test difference, to the train-val difference, and then dividing by 2. This translates   into greater consistency, and lower variance amongst the datasets.<br><br> \n",
    "\n",
    "Based on this, I chose cnn-15 as the best model.<br><br>\n",
    "    \n",
    "That said, it may be somewhat underfit, but the alternatives, model 14 is considerably overfit, and model 16 raises the train score about 3 points, but the test and val scores increase by less than a point. In addition to this, I didn't like the fact that the difference between the train and val scores (in model 16) were over ten points.</b>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ea036",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_df = best_model(18)\n",
    "best_model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3522087",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_all_recall_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59f00d",
   "metadata": {},
   "source": [
    "## Best Model Classification Report and Confusion Matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959038f",
   "metadata": {},
   "source": [
    "<b>After reviewing the 'Classification Report', and the confusion matrices (from the test data set) below, there are a few major takeaways:<br>\n",
    "\n",
    "* The model excels at correctly identifying the respective majority class. \n",
    "* The macro average of the three recall scores is .51, which is booned by the recall score of .97 for class zero. \n",
    "* For class zero, the model did great at detecting the images classified as having 'Disease Risk', with a recall score   of .97.\n",
    "* For class one, the model had trouble correctly identifying images classified as having 'Diabetic Retinopathy', with   a recall score of only .04 . \n",
    "* For class two, the model did much better at correctly identifying images classified as having 'Media Haze',with a     recall score of .53 . This despite the fact that it was the minority class by a ratio of less than 1:5 . \n",
    "* The accuracy scores based on the confusion matrices are the following:\n",
    "    \n",
    "        Disease Risk accuracy score         : .84\n",
    "        Diabetic Retinopathy accuracy score : .81\n",
    "        Media Haze accuracy score           : .87\n",
    "        Accuracy Macro Average Score        : .84\n",
    "    \n",
    "* More images, especially of 'Diabetic Retinopathy' should help improve the recall scores.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "# creating numpy array of training set predictions:\n",
    "test_preds_15 = cnn_model_15.predict(test_images)\n",
    "test_preds_15 = np.round(test_preds_15)\n",
    "\n",
    "# making sure that both 'test_preds' and 'test_labels' are type float:\n",
    "test_preds_15 = test_preds_15.astype(float)\n",
    "test_labels_15 = test_labels.astype(float)\n",
    "\n",
    "# Using a loop to divide both the 'test_preds' and 'test_labels' arrays into three parts. One for each class:\n",
    "for i in range(3):\n",
    "    globals()['test_preds_15_' + str(i)] = globals()['test_preds_15'][:,i]\n",
    "    globals()['test_labels_15_' + str(i)] = globals()['test_labels_15'][:,i]\n",
    "\n",
    "# sklearn's classification report:\n",
    "class_report = (classification_report(test_labels_15, test_preds_15))\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bd492",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "#Using a loop to create three confusion matrices. One for each class:\n",
    "for i in range(3):\n",
    "    globals()['cm_' + str(i)] = confusion_matrix(y_true=globals()[\"test_labels_15_\" + str(i)],\n",
    "                                                 y_pred=globals()[\"test_preds_15_\" + str(i)])\n",
    "\n",
    "# list with the plot labels (required as an argument)\n",
    "cm_0_plot_labels = ['Normal','Dis Risk']\n",
    "cm_1_plot_labels = ['Normal','DR']\n",
    "cm_2_plot_labels = ['Normal','MH']\n",
    "\n",
    "# plotting confusion matrix:\n",
    "plot_confusion_matrix(cm=cm_0, classes=cm_0_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm=cm_1, classes=cm_1_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm=cm_2, classes=cm_2_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efcc373",
   "metadata": {},
   "source": [
    "# Project Conclusion: Possible Further Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9a6f4",
   "metadata": {},
   "source": [
    "1. Request funding for a larger dataset to further calibrate the model\n",
    "2. Once the model is ready, we can implement it in a subset of hospitals, use the feedback to make more changes if necessary, and then expand its use from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01895e",
   "metadata": {},
   "source": [
    "# Appendix: Images for Powerpoint Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d08076",
   "metadata": {},
   "source": [
    "## Plot Image Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat = [train_df,test_df,val_df]\n",
    "big_df = pd.concat(to_concat)\n",
    "print('Disease_Risk: ',big_df['Disease_Risk'].sum())\n",
    "print('DR: ',big_df['DR'].sum())\n",
    "print('MH: ',big_df['MH'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be628b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "# creating a seperate set of labels to work with:\n",
    "train_labels_to_plt  = train_labels\n",
    "val_labels_to_plt  = val_labels\n",
    "test_labels_to_plt  = test_labels\n",
    "\n",
    "# Using a loop to divide both the 'train_preds' and 'train_labels' arrays into three parts. One for each class:\n",
    "for i in range(3):\n",
    "    globals()['train_labels_to_plt_' + str(i)] = globals()['train_labels_to_plt'][:,i]\n",
    "    globals()['val_labels_to_plt_' + str(i)] = globals()['val_labels_to_plt'][:,i]\n",
    "    globals()['test_labels_to_plt_' + str(i)] = globals()['test_labels_to_plt'][:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_to_plt_0.sum() + test_labels_to_plt_0.sum() + train_labels_to_plt_0.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_to_plt_1.sum() + test_labels_to_plt_1.sum() + train_labels_to_plt_1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_to_plt_2.sum() + test_labels_to_plt_2.sum() + train_labels_to_plt_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects = ('Disease Risk', 'Diabetic Retinopathy', 'Media Haze')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [2531,632.0,523.0]\n",
    "\n",
    "plt.bar(y_pos, performance, alpha=0.5, edgecolor='black')\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Retina Image Counts')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#This includes 1583 'normal' images, and 4273 'pneumonia' images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17b92a",
   "metadata": {},
   "source": [
    "## Lime Explanantions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f51f0",
   "metadata": {},
   "source": [
    "<b>I will:\n",
    "1. Choose three retina images. One each: normal(no disease), diabetic retinopathy('DR'), and media haze('MH'). The        images should meet the following criteria:<br>\n",
    "   1. The image class must have been predicted correctly by the model.\n",
    "   2. If 'Disease Risk'is 0, then all the columns should be 0 (the 'Totals' column should equal 0).\n",
    "   3. If either 'DR' or 'MH' are predicted, the totals column should be equal to 2, with the other predicted column being         'Disease Risk'.<br>\n",
    "\n",
    "2. Use Lime to 'explain' why a particular class was chosen.</b>\n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982da68d",
   "metadata": {},
   "source": [
    "### Selecting the Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618a27a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating a numpy array of train set image predictions based on the Best Model'(CNN-15):\n",
    "train_preds_15 = cnn_model_15.predict(train_images)\n",
    "train_preds_15 = np.round(train_preds_15).astype(int)\n",
    "\n",
    "\n",
    "# creating a list of the row numbers where the numpy arrays for the 'preds' and the 'labels' are an exact match:\n",
    "idx = 0\n",
    "new_index = []\n",
    "while idx < 1920:\n",
    "    if (train_labels[idx] == train_preds_15[idx]).sum() == 3:\n",
    "        new_index.append(idx)\n",
    "    idx+=1\n",
    "\n",
    "    \n",
    "print('train_preds_15_df:', train_preds_15.shape)\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print ('new_index length: ', len(new_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7888af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a numpy array from the original 'train' dataframe, with all the columns except 'ID'.\n",
    "\n",
    "# dropping 'ID' column from the dataframe:\n",
    "train_df_complete = train_df_original.drop(['ID'], axis=1)\n",
    "# creating a list of column names without 'ID': \n",
    "cols_list = train_df_complete.columns\n",
    "# converting dataframe to a numpy array\n",
    "train_np_complete = train_df_complete.to_numpy()\n",
    "\n",
    "\n",
    "# creating a new array that only contains the rows from train_np_complete' that match the index numbers of the\n",
    "# new_index array, then converting that array into a dataframe.\n",
    "new_arr = []\n",
    "for idx in new_index:\n",
    "    new_arr.append(train_np_complete[idx])\n",
    "preds15_new_df = pd.DataFrame(new_arr, columns = cols_list, index=new_index)\n",
    "\n",
    "\n",
    "# adding a 'Totals' column to the new dataframe, that displays the sum of all the column values for each row:\n",
    "preds15_new_df['Totals'] = preds15_new_df.sum(axis=1)\n",
    "preds15_new_df.sort_values(by=['Totals'])\n",
    "\n",
    "\n",
    "print ('preds15_new_df shape: ', preds15_new_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70819ffd",
   "metadata": {},
   "source": [
    "<b>Based on the criteria above. Using the dataframe below, I have chosen the following images for the Lime explanations:\n",
    "\n",
    " * normal image                   : image 1920\n",
    " * diabetic retinopathy image 'DR\": image 1347\n",
    " * media haze image 'DR\"          : image 657\n",
    "    \n",
    "Update: After reviewing the three images and their corresponding 'explanations', I decides to use only one of the images (diabetic retinopathy), and 'explain' the three labels for that image. \n",
    "    \n",
    "Note: the image 'ID' is one more than the image index</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b242d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds15_new_df.sort_values(by=['Totals'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae641e",
   "metadata": {},
   "source": [
    "### 'Explaining' the Images:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5504f91",
   "metadata": {},
   "source": [
    "#### View Diabetic Retinopathy Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390034df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an image of 'healthy' retina\n",
    "DR_img_1447 = image.load_img('archive/Training_Set/Training_Set/Training/1347.png',target_size=(128,128,3))\n",
    "DR_img = image.img_to_array(DR_img_1447)\n",
    "DR_img = DR_img/255\n",
    "print('diabetic retinopathy image')\n",
    "print('image shape:', DR_img.shape)\n",
    "DR_img_1447\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a LimeImageExplainer() object:\n",
    "explainer_1 = lime_image.LimeImageExplainer()\n",
    "# creating an instance of an explainer\n",
    "explanation_1 = explainer_1.explain_instance(DR_img, cnn_model_15.predict,\n",
    "                                           top_labels=3, num_samples=5000,random_seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds15_DR = cnn_model_15.predict(train_images)\n",
    "print('probabilities for each class: ',train_preds15_DR[1346])  \n",
    "print('explanation_1.top_labels: ',explanation_1.top_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d857fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs1 = plt.subplots(1, 3,sharex=False,sharey=False, figsize=(30, 6))\n",
    "labels = ['Disease Risk  (0.833)','Diabetic Retinopathy  (0.502)','Media Haze  (0.043)']\n",
    "\n",
    "for i in range (3):\n",
    "#Select the same class explained on the figures above.\n",
    "    ind =  explanation_1.top_labels[i]\n",
    "\n",
    "#Map each explanation weight to the corresponding superpixel\n",
    "    dict_heatmap = dict(explanation_1.local_exp[ind])\n",
    "    heatmap = np.vectorize(dict_heatmap.get)(explanation_1.segments) \n",
    "\n",
    "#Plot. The visualization makes more sense if a symmetrical colorbar is used.\n",
    "    retina_cmap = plt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\n",
    "    #plt.colorbar()\n",
    "    \n",
    "    axs1[i].set_title(labels[i], fontsize=20)\n",
    "    axs1[i].imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\n",
    "    plt.colorbar(retina_cmap, ax=axs1[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
